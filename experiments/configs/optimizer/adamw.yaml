# AdamW optimizer configuration
optimizer:
  type: "AdamW"
  lr: 0.001
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-08
  
scheduler:
  type: "CosineAnnealingLR"
  T_max: ${training.epochs_per_experience}
  eta_min: 1e-6