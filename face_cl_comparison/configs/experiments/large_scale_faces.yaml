# Test SLDA performance across different dataset scales
name: large_scale_faces  
description: How does SLDA scale from 40 to 150 classes?

comparison:
  # Vary only dataset scale
  vary:
    dataset.name:
      - olivetti   # 40 classes (baseline)
      - lfw_12     # 12 classes (very high quality data)
      - lfw_30     # 30 classes 
      - lfw_60     # 60 classes
      - lfw_90     # 90 classes
      - lfw_120    # 120 classes

fixed:
  # Fix everything else
  strategy:
    name: slda
    params:
      shrinkage_param: 0.0001
      streaming_update_sigma: true
  
  model:
    backbone:
      name: efficientnet_b2
      pretrained: true
    
  dataset:
    n_experiences: 10
    
  training:
    epochs_per_experience: 1  # SLDA uses 1 epoch
    batch_size: 32
    lr: 0.001
    debug: false
    seeds: [42, 123, 456]
    
  evaluation:
    primary_metric: average_accuracy
    track_metrics:
      - training_time
      - memory_usage
      - accuracy_per_experience

# Analysis focus:
# - How does performance scale with number of classes?
# - Is there a sweet spot for number of experiences?
# - Memory requirements for different scales
# - When do we need larger models?